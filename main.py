"""
Solve Amazon Challenge using Inverse Optimization.

Author: Pedro Zattoni Scroccaro
"""

import sys
import pickle
from utils import (
    route_to_zone_seq, solve_ATSP, zone_seq_to_vec, amazon_score, zone_centers
)
import numpy as np
import time
import argparse
import invopt as iop


# %%%%%%%%%%%%%%%%%%%%%%%%%%% Simulation paramters %%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Checks if the script in being run through command-line or as a script in an
# IDE, e.g., Spyder
if len(sys.argv) > 1:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        'station_code',
        choices=[
            'DLA7', 'DLA9', 'DLA8', 'DBO3', 'DSE5', 'DSE4', 'DCH4', 'DBO2',
            'DCH3', 'DLA3', 'DLA4', 'DAU1', 'DCH1', 'DLA5', 'DSE2', 'DCH2',
            'DBO1'
        ], help='Station (a.k.a. depot) code.'
    )
    parser.add_argument(
        'solver_IO', choices=['gurobi', 'ortools', 'greedy'],
        help=('Solver used for the IO algorithm.')
    )
    parser.add_argument(
        'solver_zone_seq',  choices=['gurobi', 'ortools', 'greedy'],
        help=(
            'Solver used to compute the zone sequence when computing the'
            'Amazon score.'
        )
    )
    parser.add_argument(
        'solver_complete_route', choices=['gurobi', 'ortools', 'greedy'],
        help=(
            'Solver used to compute the complete route when computing the'
            'Amazon score.'
        )
    )
    parser.add_argument(
        'step_size_constant', type=float,
        help=(
            'Constant that multiplies the step-size used for the IO algorithm.'
        )
    )
    parser.add_argument(
        'T', type=int, help=(
            'Number of epochs of the IO algorithm. For each epoch, the IO'
            'algorithm uses N training examples, where N is the size of the'
            'training dataset.'
        )
    )
    parser.add_argument(
        'resolution', type=int,
        help=(
            'The Amazon score of the learned model is computed every'
            'RESOLUTION epoch(s) of the IO algorithm.'
        )
    )
    parser.add_argument(
        'update_step', choices=['standard', 'exponentiated'],
        help=('Type of update step of the IO algorithm.')
    )
    parser.add_argument(
        'batch_type',
        help=(
            'If float, it is the fraction of the dataset used to compute a'
            'stochastic subgradients of the loss function. For example, if'
            'batch_type=b, 100*b% of the data is used to compute the'
            'subgradients. If batch_type=\'reshuffled\', uses reshuffled'
            'sampling strategy.'
        )
    )
    parser.add_argument(
        'averaged_type', type=int, choices=[0, 1, 2],
        help=(
            'Type of averaging of the iterates. See invopt documentation'
            'for more details.'
        )
    )
    parser.add_argument(
        'dataset_size', type=float,
        help=('Percentage of the training dataset used.')
    )
    parser.add_argument(
        'initial_theta',
        help=(
            'Type of initial theta vector. Possible choices are: zone_centers,'
            'which sets theta as the Euclidean distance between the zone'
            'centers, or uniform_X, which sets theta as a uniform vector'
            'with sum(theta) = X.'
        )
    )
    parser.add_argument(
        'path_to_input_data',
        help=(
            'Path to the folder where the pre-processed Amazon Challenge data'
            'is located.'
        )
    )
    parser.add_argument(
        'path_to_output_data',
        help=('Path to the folder where the results should be saved.')
    )
    parser.add_argument(
        '--sub_loss', action='store_true',
        help=('Use Suboptimality Loss (SL) instead of Augmented SL.')
    )
    parser.add_argument(
        '--compute_train_score', action='store_true',
        help=(
            'Also compute the Amazon score (and possibly the prediction error)'
            'for the training data, i.e., the in-sample score.'
        )
    )
    parser.add_argument(
        '--area_cluster', action='store_true',
        help=(
            'Assumes expert respects area clusters, and enforce this behaviour'
            'in the solution.'
        )
    )
    parser.add_argument(
        '--region_cluster', action='store_true',
        help=(
            'Assumes expert respects region clusters, and enforce this'
            'behavior in the solution.'
        )
    )
    parser.add_argument(
        '--zone_id_diff', action='store_true',
        help=(
            'Assumes expert respects the one unit difference rule, and enforce'
            'this behavior in the solution.'
        )
    )
    parser.add_argument(
        '--compute_error', action='store_true',
        help=(
            'Compute prediction error of the zone sequences generated by the'
            'learned IO cost vector.'
        )
    )
    args = parser.parse_args()

    station_code = args.station_code
    solver_IO = args.solver_IO
    solver_zone_seq = args.solver_zone_seq
    solver_complete_route = args.solver_complete_route
    step_size_constant = args.step_size_constant
    T = args.T
    resolution = args.resolution
    update_step = args.update_step
    batch_type = args.batch_type
    averaged_type = args.averaged_type
    dataset_size = args.dataset_size
    initial_theta = args.initial_theta
    path_to_input_data = args.path_to_input_data
    path_to_output_data = args.path_to_output_data
    sub_loss = args.sub_loss
    compute_train_score = args.compute_train_score
    area_cluster = args.area_cluster
    region_cluster = args.region_cluster
    zone_id_diff = args.zone_id_diff
    compute_error = args.compute_error
else:
    # ['DLA7', 'DLA9', 'DLA8', 'DBO3', 'DSE5', 'DSE4', 'DCH4', 'DBO2', 'DCH3',
    #  'DLA3', 'DLA4', 'DAU1', 'DCH1', 'DLA5', 'DSE2', 'DCH2', 'DBO1']
    station_code = 'DBO1'
    solver_IO = 'gurobi'
    solver_zone_seq = 'gurobi'
    solver_complete_route = 'ortools'
    step_size_constant = 0.0005
    T = 2
    resolution = 1
    update_step = 'standard'
    batch_type = 'reshuffled'
    averaged_type = 0
    dataset_size = 1
    initial_theta = 'zone_centers'
    sub_loss = False
    compute_train_score = True
    area_cluster = True
    region_cluster = True
    zone_id_diff = True
    compute_error = True

    path_to_input_data = 'path/to/input/data/'
    path_to_output_data = 'path/to/output/data/'


print(f'station_code = {station_code}')
print(f'solver_IO = {solver_IO}')
print(f'solver_zone_seq = {solver_zone_seq}')
print(f'solver_complete_route = {solver_complete_route}')
print(f'step_size_constant = {step_size_constant}')
print(f'T = {T}')
print(f'resolution = {resolution}')
print(f'update_step = {update_step}')
print(f'batch_type = {batch_type}')
print(f'averaged_type = {averaged_type}')
print(f'dataset_size = {dataset_size}')
print(f'initial_theta = {initial_theta}')
print(f'sub_loss = {sub_loss}')
print(f'compute_train_score = {compute_train_score}')
print(f'area_cluster = {area_cluster}')
print(f'region_cluster = {region_cluster}')
print(f'zone_id_diff = {zone_id_diff}')
print(f'compute_error = {compute_error}')


# %%%%%%%%%%%%%%%%%%%%%%%%%%% Load data %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

print('')
print('Loading data...')
tic = time.time()
dataset_train = pickle.load(
    open(path_to_input_data + station_code + '_train.p', "rb")
)
dataset_test = pickle.load(
    open(path_to_input_data + station_code + '_test.p', "rb")
)
toc = time.time()
print(f'Done! ({round(toc-tic, 2)} seconds)')


# %%%%%%%%%%%%%%%%%%%%%%%%%%% Process data %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Extract zone_ids from training dataset and create dict of all stops per zone.
zone_ids_train = set()
for route_ID in dataset_train:
    stops = dataset_train[route_ID]['stops']
    for stop in stops:
        zone_id = stops[stop]['zone_id']
        zone_ids_train.add(zone_id)

m_train = len(zone_ids_train)

# Zone centers
zc_train, _ = zone_centers(dataset_train)

# Associate every zone ID (str) with a unique index (int). This makes it easier
# to work with the zones. Depot gets index=0
list_zones = list(zone_ids_train)
list_zones.insert(0, list_zones.pop(list_zones.index('depot')))
m_total = len(list_zones)
indexes = list(range(m_total))
zone_id_to_index = dict(zip(list_zones, indexes))
index_to_zone_id = dict(zip(indexes, list_zones))

# Create zone-to-area dict
index_to_area = {
    zone_id_to_index[zone]: zone[:-2]+zone[-1] for zone in list_zones
}

# Create zone-to-region dict
index_to_region = {zone_id_to_index[zone]: zone[:-3] for zone in list_zones}

# Create IO datasets of signal and expert response
dataset_train_z = []
for route_ID in dataset_train:
    zone_seq = route_to_zone_seq(
        dataset_train[route_ID]['stops'], zone_id_to_index
    )
    dataset_train_z.append(((set(zone_seq), m_train), zone_seq))


# %%%%%%%%%%%%%%%%%%%%%%%%%%% IO functions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# See InvOpt package documentation for details:
# https://github.com/pedroszattoni/invopt

def step_size_func(t):
    """Step-size function."""
    return step_size_constant/(1+t)


def phi(s, x):
    """Feature mapping."""
    _, m = s
    x_vec = zone_seq_to_vec(x, m)
    return x_vec


def FOP(theta, s):
    """Forward optimization problem."""
    zones_set, m = s

    if len(theta) != m**2:
        raise Exception(
            'Length of theta vector does not match number of zones!'
        )

    theta_mat = theta.reshape(m, m)

    dists = {}
    for i in zones_set:
        for j in zones_set:
            if i != j:
                # Enforcer clustering of areas
                if area_cluster:
                    M_A = (index_to_area[i] != index_to_area[j])
                else:
                    M_A = 0

                # Enforcer clustering of regions
                if region_cluster:
                    M_R = (index_to_region[i] != index_to_region[j])
                else:
                    M_R = 0

                # Penalty of 1 for each token that changes by more than one
                # zone ID = W-x.yZ
                if zone_id_diff:
                    zone_id_i = index_to_zone_id[i]
                    zone_id_j = index_to_zone_id[j]

                    W_i = ord(zone_id_i[0])
                    W_j = ord(zone_id_j[0])
                    M_W = (np.abs(W_i - W_j) > 1.5)

                    try:
                        x_i = int(zone_id_i[2:-3])
                        x_j = int(zone_id_j[2:-3])
                        M_x = (np.abs(x_i - x_j) > 1.5)
                    except ValueError:
                        M_x = 1

                    try:
                        y_i = int(zone_id_i[-2])
                        y_j = int(zone_id_j[-2])
                        M_y = (np.abs(y_i - y_j) > 1.5)
                    except ValueError:
                        M_y = 1

                    Z_i = ord(zone_id_i[-1])
                    Z_j = ord(zone_id_j[-1])
                    M_Z = (np.abs(Z_i - Z_j) > 1.5)

                    M_token = M_W + M_x + M_y + M_Z
                else:
                    M_token = 0

                dists[(i, j)] = M_A + M_R + M_token + theta_mat[i, j]

    x_opt, _ = solve_ATSP(dists, solver_IO)

    return x_opt


def FOP_aug(theta, s_hat, x_hat):
    """Augmented FOP."""
    _, m = s_hat
    x_hat_vec = zone_seq_to_vec(x_hat, m)

    theta_aug = theta - (1 - 2*x_hat_vec)
    x_aug = FOP(theta_aug, s_hat)

    return x_aug


def callback(theta):
    """Store iterate."""
    return theta


# %%%%%%%%%%%%%%%%%%%%%%%%%%% IO training/testing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Compute initial theta
p = m_train**2
if initial_theta == 'zone_centers':
    theta_0 = np.ones((m_train, m_train))
    for i in range(m_train):
        for j in range(m_train):
            zone_id_i = index_to_zone_id[i]
            zone_id_j = index_to_zone_id[j]
            dist_lat = zc_train[zone_id_i][0] - zc_train[zone_id_j][0]
            dist_lng = zc_train[zone_id_i][1] - zc_train[zone_id_j][1]
            theta_0[i][j] = np.linalg.norm([dist_lat, dist_lng])
    theta_0 = theta_0.flatten()
elif initial_theta[:7] == 'uniform':
    sum_theta = float(initial_theta[8:])
    theta_0 = (sum_theta/p)*np.ones(p)

score_train_hist = []
score_test_hist = []
error_train_hist = []
error_test_hist = []

print('')
print('Inverse optimization...')
tic = time.time()
if sub_loss:
    FOP_FOM = FOP
else:
    FOP_FOM = FOP_aug

n_training_examples = round(dataset_size*len(dataset_train_z))

# If batch_type is not 'reshuffled', transform T from epochs to total number
# of iterations.
if batch_type != 'reshuffled':
    batch_type = float(batch_type)
    T = T*n_training_examples
    resolution = resolution*n_training_examples

theta_IO_list = iop.FOM(dataset_train_z[:n_training_examples],
                        phi, theta_0, FOP_FOM, step_size_func, T,
                        Theta='nonnegative',
                        step=update_step,
                        batch_type=batch_type,
                        averaged=averaged_type,
                        callback=callback,
                        callback_resolution=resolution)
toc = time.time()
print(f'Done! ({round(toc-tic, 2)} seconds)')

print('')
print('Computing Amazon scores...')
tic = time.time()
for theta_IO in theta_IO_list:
    if compute_train_score:
        results_train = amazon_score(
            theta_IO, dataset_train, zc_train, zone_id_to_index,
            solver_complete_route, solver_zone_seq, station_code, area_cluster,
            region_cluster, zone_id_diff, compute_error
        )
        if compute_error:
            scores_train, error_train = results_train
            score_train_hist.append(scores_train['submission_score'])
            error_train_hist.append(error_train)

    results_test = amazon_score(
        theta_IO, dataset_test, zc_train, zone_id_to_index,
        solver_complete_route, solver_zone_seq, station_code, area_cluster,
        region_cluster, zone_id_diff, compute_error
    )
    if compute_error:
        scores_test, error_test = results_test
        score_test_hist.append(scores_test['submission_score'])
        error_test_hist.append(error_test)

toc = time.time()
print(f'Done ({round(toc-tic, 2)} seconds)')
print('')


# %%%%%%%%%%%%%%%%%%%%%%%%%%% Log experiment %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

current_time = time.localtime()
time_identifier = (
    f'{current_time.tm_year}{current_time.tm_mon:02d}'
    f'{current_time.tm_mday:02d}{current_time.tm_hour:02d}'
    f'{current_time.tm_min:02d}{current_time.tm_sec:02d}'
)

with open(path_to_output_data+str(time_identifier)+'_log.txt', 'a') as log:
    log.write(f'station_code = {station_code}\n')
    log.write(f'solver_IO = {solver_IO}\n')
    log.write(f'solver_zone_seq = {solver_zone_seq}\n')
    log.write(f'solver_complete_route = {solver_complete_route}\n')
    log.write(f'step_size_constant = {step_size_constant}\n')
    log.write(f'T = {T}\n')
    log.write(f'resolution = {resolution}\n')
    log.write(f'update_step = {update_step}\n')
    log.write(f'batch_type = {batch_type}\n')
    log.write(f'averaged_type = {averaged_type}\n')
    log.write(f'dataset_size = {dataset_size}\n')
    log.write(f'initial_theta = {initial_theta}\n')
    log.write(f'sub_loss = {sub_loss}\n')
    log.write(f'compute_train_score = {compute_train_score}\n')
    log.write(f'area_cluster = {area_cluster}\n')
    log.write(f'region_cluster = {region_cluster}\n')
    log.write(f'zone_id_diff = {zone_id_diff}\n')
    log.write(f'compute_error = {compute_error}\n')

results = {}

if compute_train_score:
    results['score_train_hist'] = score_train_hist
    results['error_train_hist'] = error_train_hist
    results['len_train'] = len(dataset_train)
results['score_test_hist'] = score_test_hist
results['error_test_hist'] = error_test_hist
results['len_test'] = len(dataset_test)

pickle.dump(
    results,
    open(path_to_output_data + str(time_identifier) + '_results.p', "wb")
)

print('time_identifier = ' + str(time_identifier))
